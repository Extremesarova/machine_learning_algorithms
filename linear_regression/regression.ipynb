{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "## 0. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - What is Linear Regression?\n",
    " - Which methods are used for optimization?\n",
    "     - When is it suitable to use quantitative approach (gradient descent) instead of analytical solution?\n",
    "     - What is analytical solution? How to derive the formula?\n",
    "     - What is gradient descent and how can it be used to optimize parameters?\n",
    " - Which metrics are used to evaluate the performance of a regression model?\n",
    "     - Describe MAE, MSE, RMSE, MAPE, MedAE\n",
    " - What is $R^2$ (coefficient of determination)?\n",
    "     - In which case $R^2$ can be lower than 0?\n",
    " - What is regularization? And why is it needed?\n",
    "     - What is Lasso?\n",
    "     - What is Ridge?\n",
    "     - What is the difference between Lasso and Ridge?\n",
    "     - What is Elastic Net?\n",
    "     - How to find the regularization parameter in LASSO or Ridge or Elastic Net?\n",
    "     - In what situations LASSO performs better than Ridge?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, let's import needed modules and set random seed (we'll use it if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from linear_regression_implementations import SciPyLinearRegressionOLS, LinearRegressionOLS, SciPyLinearRegressionNNOLS\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading california housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into training and testing dataset. 10% for test set will be enough, because we have enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.scaler import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Predictive modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Analytical solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First solution will use the analytical solution to OLS to get the weights/coefficients: $\\hat{\\beta} = (X^TX)^{-1}(X^TY)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Wrapper for analytical solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting we should include first column of 1's to `X_train_scaled` matrix and check the $det(X^TX)$ - it should not be equal to 0 - otherwise we would not be able to invert the matrix $(X^TX)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [ 0.84256988  0.11978692 -0.27791665  0.32476338 -0.00115151 -0.03999763\n",
      " -0.89935429 -0.87044066]\n",
      "The intercept is 2.0729521328596032\n"
     ]
    }
   ],
   "source": [
    "ols_reg = LinearRegressionOLS()\n",
    "ols_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {ols_reg.coef_}\")\n",
    "print(f\"The intercept is {ols_reg.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.1.2 Sklearn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [ 0.84256988  0.11978692 -0.27791665  0.32476338 -0.00115151 -0.03999763\n",
      " -0.89935429 -0.87044066]\n",
      "The intercept is 2.072952132859605\n"
     ]
    }
   ],
   "source": [
    "sklearn_ols_reg = LinearRegression(positive=False)\n",
    "sklearn_ols_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {sklearn_ols_reg.coef_}\")\n",
    "print(f\"The intercept is {sklearn_ols_reg.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Wrapper for `scipy.linalg.lstsq` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see the results are the same, because sklearn is using <code>scipy.linalg.lstsq</code> method under the hood. Which, also, wrapped by me below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [ 0.84256988  0.11978692 -0.27791665  0.32476338 -0.00115151 -0.03999763\n",
      " -0.89935429 -0.87044066]\n",
      "The intercept is 2.0729521328596046\n"
     ]
    }
   ],
   "source": [
    "scipy_ols_reg = SciPyLinearRegressionOLS()\n",
    "scipy_ols_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {scipy_ols_reg.coef_}\")\n",
    "print(f\"The intercept is {scipy_ols_reg.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results are expectedly the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non-negative least-squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would want to constrain the coefficients to non-negative values whenever a negative value makes no physical sense, say because it represents the intensity of a pixel, or the price of an object, or a frequency count, or a chemical concentration, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Sklearn's implementation for non-negative least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When <code>positive</code> set to <code>True</code>, during initialization of <code>sklearn.linear_model.LinearRegression</code> object, it will force the coefficients to be positive.\n",
    "\n",
    "We can see that the solution looks completely different - all the coefficients are either zero or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [0.82392238 0.23238543 0.         0.0242489  0.0405112  0.\n",
      " 0.         0.        ]\n",
      "The intercept is 2.0729521328596037\n"
     ]
    }
   ],
   "source": [
    "sklearn_ols_reg_pos = LinearRegression(positive=True)\n",
    "sklearn_ols_reg_pos.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {sklearn_ols_reg_pos.coef_}\")\n",
    "print(f\"The intercept is {sklearn_ols_reg_pos.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Wrapper for `scipy.optimize.nnls` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which is naturally equal to the results from the wrapped method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [0.82392238 0.23238543 0.         0.0242489  0.0405112  0.\n",
      " 0.         0.        ]\n",
      "The intercept is 2.0729521328596245\n"
     ]
    }
   ],
   "source": [
    "scipy_nn_ols_reg = SciPyLinearRegressionNNOLS()\n",
    "scipy_nn_ols_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {scipy_nn_ols_reg.coef_}\")\n",
    "print(f\"The intercept is {scipy_nn_ols_reg.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Gradient Descent solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent (GD) or Stochastic Gradient Descent (SGD) solution is needed for regression problems with a large number of training samples and features (> 100 000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [ 0.45362911  0.11823577  0.35700974 -0.45554855  0.03781292 -2.67187916\n",
      " -1.44594935 -1.35306023]\n",
      "The intercept is [2.0188528]\n"
     ]
    }
   ],
   "source": [
    "sklearn_sgd_reg = SGDRegressor()\n",
    "sklearn_sgd_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {sklearn_sgd_reg.coef_}\")\n",
    "print(f\"The intercept is {sklearn_sgd_reg.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The $R^2$ is the coefficient of determination which is defined as $R^2 = 1 -\\frac{\\sum\\limits _{i = 1} ^N (y_i - \\hat y_i)}{\\sum\\limits _{i = 1} ^N (y_i - \\bar y_i)} = 1 - \\frac{SS_{fit}}{SS_{mean}}$. <br>\n",
    "It shows how much the fit of the chosen model is better than a fit of a horizontal straight line (mean value for target $y$). <br>\n",
    "The best possible score is 1.0, and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of $y$, disregarding the input features, would get a $R^2$ score of 0.0. <br>\n",
    "$R^2$ can have a negative value without violating any rules of math. $R^2$ is negative only when the chosen model does not follow the trend of the data, so fits worse than a $y$-mean line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the train data is equal to 0.61\n",
      "R2 on the test data is equal to 0.6\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "print(f\"R2 on the train data is equal to {lr.score(X_train_scaled, y_train).round(2)}\")\n",
    "print(f\"R2 on the test data is equal to {lr.score(X_test_scaled, y_test).round(2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TODO:\n",
    " - the idea of regularization and why is it needed\n",
    " - using gradient descent instead of using formula (analytical solution)\n",
    " - comparing results of SGDRegressor with results of ridge regression\n",
    " - in which situations using gradient descent is more advisable\n",
    " - Linear regression metrics\n",
    "      - R2, adjusted R2 and statistical significance\n",
    " - Non-linear methods for regression (decision trees and random forest regressors)\n",
    " - Write down questions about regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_from_scratch",
   "language": "python",
   "name": "ml_from_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
